{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9cjyKcNzU8i"
   },
   "source": [
    "## Vector Databases\n",
    "\n",
    "<img src=\"./assets/vector_banner.jpg\" height=\"25%\">\n",
    "\n",
    "We started this workshop with **text representation** as one of the key components of any NLP system.\n",
    "As we progressed from simple Bag of Words setup to highly contextualised Transformer models, we now have rich & dense representations.\n",
    "The utility of such representations also increased multifold from word/sentence representations to features that can used for a number of downstream tasks.\n",
    "\n",
    "These representations, also called as vectors or embedding vectors are long series of numbers. Their retrieval and persistence requires specialised database management systems called **Vector Databases**.\n",
    "\n",
    "Vector Databases are particularly suited for handling data in the form of vectors, embeddings, or feature representations, which are commonly used in various applications like machine learning, natural language processing, computer vision, and recommendation systems.\n",
    "\n",
    "Key Features:\n",
    "- High-dimensional Data Support\n",
    "- Similarity Search\n",
    "- Indexing Techniques\n",
    "- Dimensionality Reduction\n",
    "\n",
    "There are a number of different off-the-shelf options available, such as:\n",
    "- [ChromaDB](https://www.trychroma.com/)\n",
    "- [PineCone](https://www.pinecone.io/)\n",
    "- [Milvus](https://milvus.io/)\n",
    "- [Weaviate](https://weaviate.io/)\n",
    "- [AeroSpike](https://aerospike.com/)\n",
    "- [OpenSearch](https://opensearch.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_luN80B2Am7"
   },
   "source": [
    "## Let us Begin with Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_04/02_vector_databases_hf_inference_endpoint.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z_dNazilzRUF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# install dependencies\n",
    "# !pip install -q chromadb\n",
    "# !pip install retry\n",
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q960w1bz2Am9"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GV6LIKdMBy2r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from retry import retry\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD7AixSz2Am_"
   },
   "source": [
    "## HuggingFace Inference EndPoints ðŸ¤—\n",
    "\n",
    "Another key offering from HuggingFace is *[Inference Endpoints](https://huggingface.co/inference-endpoints)*.\n",
    "These endpoints provide access to hundreds of large models hosted on HuggingFace infra for easy use.\n",
    "\n",
    "All you need is a quick [sign-up](https://huggingface.co/login) and an API Key and bingo!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOT-wEBZ2AnA"
   },
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "This is an amazing python framework initially proposed along with the seminal paper titled [Sentence-BERT](https://www.sbert.net/).\n",
    "It provides clean high-level interfaces to easily use Language Models for computing text embeddings for various use-cases.\n",
    "\n",
    "In this notebook we will leverage pretrained models supported by sentence transformer rather than directly using the package.\n",
    "\n",
    "There is a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) now maintained to keep track of the state-of-the-art embedding models called the **Massive Text Embedding Benchmark (MTEB) Leaderboard**\n",
    "\n",
    "<img src=\"./assets/mteb.png\">\n",
    "\n",
    "> Source : [HuggingFace](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10G0QGLn2AnA"
   },
   "source": [
    "## MPNET Model\n",
    "\n",
    "- This model transforms sentences/paragraphs to a 768 dimensional vector space and is optimised for question-answering tasks.\n",
    "- The model card is available [here](https://huggingface.co/pinecone/mpnet-retriever-discourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "xr7GrCDXCHKM"
   },
   "outputs": [],
   "source": [
    "EMB_MODEL_ID = 'pinecone/mpnet-retriever-discourse'\n",
    "HF_TOKEN = '<YOUR-TOKEN>'\n",
    "EMB_API_URL = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{EMB_MODEL_ID}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA9LV_E2AnB"
   },
   "source": [
    "## Embeddings using ðŸ¤— Inference Endpoint\n",
    "- We setup a utility function that takes a list of sentences as input and generates embeddings as response\n",
    "- We use the ``retry`` package to allow for sufficient time and retries for the APIs to respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=HF_TOKEN,\n",
    "    model_name=EMB_MODEL_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MqxEofYVCqqz"
   },
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "        \"Another key offering from HuggingFace is Inference Endpoints. These endpoints provide access to hundreds of large models hosted on HuggingFace infra for easy use.\",\n",
    "        \"This is an amazing python framework initially proposed along with the seminal paper titled Sentence-BERT. It provides clean high-level interfaces to easily use Language Models for computing text embeddings for various use-cases.\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5249432325363159,\n",
       " -0.04365385323762894,\n",
       " 0.5124771595001221,\n",
       " 0.21908265352249146,\n",
       " 0.4560490548610687]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate embeddings\n",
    "sample_emb = huggingface_ef(sample_texts[0])\n",
    "sample_emb[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqCfZvSRS5zn",
    "outputId": "a56d0831-f7ea-4c2e-eb05-c9d0690c72da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embedding length\n",
    "len(sample_emb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nph6RzJbDsEx"
   },
   "source": [
    "## Vector Database: ChromaDB\n",
    "\n",
    "As mentioned above, there are a number of offering available. For this workshop we will make use of\n",
    "[ChromaDB](https://www.trychroma.com/).\n",
    "\n",
    "It is a super simple setup which is easy to use. The following figure showcases the overall flow\n",
    "\n",
    "<img src=\"./assets/chroma_workflow.png\">\n",
    "\n",
    "> Source :[chromadb](https://docs.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubFv6W-C2AnC"
   },
   "source": [
    "### Create an Instance of the Database Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GVL3ByK9ZG76"
   },
   "outputs": [],
   "source": [
    "# in memory\n",
    "chroma_client = chromadb.Client()\n",
    "# save to disk: client = chromadb.PersistentClient(path=\"/path/to/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vJV9I0dJDB8G"
   },
   "outputs": [],
   "source": [
    "def create_db_and_load_data(chroma_client,collection_name, embedding_func, documents):\n",
    "  db = chroma_client.create_collection(name=collection_name,\n",
    "                                       embedding_function=embedding_func)\n",
    "  for i,d in enumerate(documents):\n",
    "    db.add(\n",
    "      documents=d,\n",
    "      ids=str(i)\n",
    "    )\n",
    "  return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFAsZHnYEi8g"
   },
   "source": [
    "## Insert Data\n",
    "\n",
    "Now that we have a utility to interact with the vector database, let us add some data to it and check how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-h5HecZrEKTa"
   },
   "outputs": [],
   "source": [
    "db = create_db_and_load_data(chroma_client=chroma_client,\n",
    "                             collection_name=\"llm_workshop\",\n",
    "                             embedding_func=huggingface_ef,\n",
    "                             documents=sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data', 'included'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.peek().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "Sm_zGs8HEc2F",
    "outputId": "169b82d0-c268-4e03-b40d-ad8cda697f43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.5249432325363159, -0.04365385323762894, 0....</td>\n",
       "      <td>Another key offering from HuggingFace is Infer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.5217247605323792, 0.5370820760726929, -0.2...</td>\n",
       "      <td>This is an amazing python framework initially ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ids                                         embeddings  \\\n",
       "0   0  [-0.5249432325363159, -0.04365385323762894, 0....   \n",
       "1   1  [-0.5217247605323792, 0.5370820760726929, -0.2...   \n",
       "\n",
       "                                           documents  \n",
       "0  Another key offering from HuggingFace is Infer...  \n",
       "1  This is an amazing python framework initially ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = db.peek()\n",
    "pd.DataFrame.from_dict({k:v for k,v in results.items() if k in['ids','documents','embeddings']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGgZzOzYElr5"
   },
   "source": [
    "## Retrieve Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "N8CeEIX3aXiE"
   },
   "outputs": [],
   "source": [
    "question = \"HuggingFace Key Offering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGe1IwAzX98K",
    "outputId": "2d19d4fb-2a7e-4e86-caf2-4f71123dca4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['0']],\n",
       " 'distances': [[169.98219299316406]],\n",
       " 'metadatas': [[None]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Another key offering from HuggingFace is Inference Endpoints. These endpoints provide access to hundreds of large models hosted on HuggingFace infra for easy use.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(query_texts=[question], n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "qFRSZWHcEfEd"
   },
   "outputs": [],
   "source": [
    "def get_relevant_documents(query, db):\n",
    "  relevant_doc = db.query(query_texts=[query], n_results=1)['documents'][0][0]\n",
    "  return relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GfucsODPEusp",
    "outputId": "9d0f89c1-0b1d-4337-9719-e9b390f0e8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Another key offering from HuggingFace is Inference Endpoints. These endpoints provide access to hundreds of large models hosted on HuggingFace infra for easy use.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search using embeddings\n",
    "get_relevant_documents(question, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_9LRJZgMDs1"
   },
   "source": [
    "## HuggingFace Powered Question Answering Setup\n",
    "\n",
    "Similar to Embedding Endpoints, HF also provides us with capabilities to directly leverage models for tasks such as:\n",
    "- Text Generation\n",
    "- Question Answering, etc.\n",
    "\n",
    "We can leverage local setups like GPT4ALL with LangChain, OpenAI APIs or even HuggingFace transformers as well. For this exercise, we will focus on leveraging **HuggingFace Endpoints** for **QA tasks** itself.\n",
    "\n",
    "We will make use of [Roberta-Base-Squad2](https://huggingface.co/deepset/roberta-base-squad2) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "KCFidBCIE0i_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'Bearer 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'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_MODEL_ID = 'deepset/roberta-base-squad2'\n",
    "QA_API_URL = f\"https://api-inference.huggingface.co/models/{QA_MODEL_ID}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "HEADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "3sOqsmAkMJVK"
   },
   "outputs": [],
   "source": [
    "def get_answer(question,context):\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"context\":context\n",
    "    }\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", QA_API_URL, headers=HEADERS, data=data)\n",
    "    try:\n",
    "      decoded_response = json.loads(response.content.decode(\"utf-8\"))\n",
    "      return decoded_response#decoded_response['answer'], decoded_response['score'], \"\"\n",
    "    except Exception as ex:\n",
    "      return \"Apologies but I could not find any relevant answer\", 0.0, ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IU-il-z2AnE",
    "outputId": "820acf52-12eb-48eb-88e9-9ab7a29f7670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Key Offering\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LiCiEjwvVNGI",
    "outputId": "fc0930d9-4d09-4aa4-9ec6-ea5c2c154f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Another key offering from HuggingFace is Inference Endpoints. These endpoints provide access to hundreds of large models hosted on HuggingFace infra for easy use.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = get_relevant_documents(question, db)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKMvj0aCNs2U",
    "outputId": "312d878f-2de9-45e9-ed36-abfbd6f15e05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Inference Endpoints', 0.1849050521850586, '')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(question,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLUe4RmN2IMP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
