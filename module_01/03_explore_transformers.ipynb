{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ab9deb-d5cd-45d4-a4be-4748d15df4e5",
   "metadata": {},
   "source": [
    "## Exploring Transformer Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd7bf0-01eb-42fa-86a7-0614c6948ed9",
   "metadata": {},
   "source": [
    "## The RNN Limitation\n",
    "The RNN layer (LSTM, or GRU, etc.) takes in a context window of a defined size as input and encodes all of it into a single vector. This bottleneck vector needs to capture a lot of information in itself before the decoding stage can use it to start generating the next token. To enhance performance of sequence to sequence tasks a typical Encoder-Decoder architecture is the go-to choice.\n",
    "\n",
    "<img src=\"./assets/encoder_decoder_notebook_3.png\">\n",
    "\n",
    "Let us consider the case of **Machine Translation**, i.e. translation of English to Spanish (or any other language).\n",
    "\n",
    "In a typical __Encoder-Decoder__ architecture, the Encoder takes in the input text in English as input and prepares a condensed vector representation of the whole input. Typically termed as bottleneck features. The Decoder then uses these features to generate the translated text in Spanish.\n",
    "\n",
    "While this architecture and its variants worked wonders, they had issues. Issues such as inability handle longer input sequences, cases where there is not a one to one mapping between input vs output language and so on.\n",
    "\n",
    "To handle these issues, __Vasvani et. al.__ in their now famouly titled paper __Attention Is All You Need__ build up on the concepts of attention. The main highlight of this work was the Transformer architecture. Transformers were shown to present state of the art results on multiple benchmarks without using any recurrence or convolutional components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97031ae-f2ee-47f5-8873-062bc1da55f7",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "- The transformer architecture was presented in the seminal paper __Attention is All You Need__ by Vaswani et al. back in 2017\n",
    "- A transformer is a __recurrence-__ and __convolution-free__ attention-based encoder-decoder architecture\n",
    "- Introduced the concept of multi-head attention and positional encodings\n",
    "- Also revolutionalised Computer Vision domain (see ViT)\n",
    "\n",
    "\n",
    "<img src=\"./assets/transformer_arch_notebook_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a3aab-a33b-4864-a5cc-f175146af5ce",
   "metadata": {},
   "source": [
    "## Attention is All you Need ⚠️\n",
    "\n",
    "\n",
    "### Attention to the Rescue\n",
    "Attention is one of the most powerful concepts in the deep learning space that really changed the game. The core idea behind the attention mechanism is to make use of all interim hidden states of the RNN to decide which one to focus upon before it is used by the decoding stage. \n",
    "\n",
    "### Contextual Embeddings\n",
    "The [TagLM architecture by Peters et al. in 2017](https://arxiv.org/abs/1705.00108) was one of the first works that provided an insight into how we could combine __pre-trained word embeddings__ with a __pre-trained neural language model__ to generate __context-aware embeddings__ for downstream NLP tasks.\n",
    "\n",
    "The big breakthrough that changed the NLP landscape came in the form of __ELMo, or Embeddings from Language Models__. The ELMo architecture was presented by Peters et al. in their work titled [__Deep Contextualized Word Representations in 2018__](https://arxiv.org/abs/1802.05365). Without going into too much detail, the main highlights of the ELMo architecture were:\n",
    "\n",
    "- The model used a bi-LSTM-based language model.\n",
    "- Character CNNs were used to generate embeddings, in place of pre-trained word vectors, which made use of huge 4096 LSTM units but transformed into smaller 512-sized vectors using feedforward layers.\n",
    "- The main innovation was to make use of all the hidden bi-LSTM layers for generating input representation. Unlike previous works, where only the final LSTM layer was used to fetch the representation of the input, this work took a weighted average of all the hidden layers' hidden states. This helped the model learn contextual word embeddings where each layer contributed to things like syntax and semantics.\n",
    "\n",
    "### Self-Attention\n",
    "- Self-attention was proposed by Cheng et al. in their paper titled Long Short-Term Memory Networks for Machine Reading in 2016\n",
    "- Self-attention enables a model to learn the correlation between the current token (character or word or sentence, etc.) and its context window. In other words, it is an attention mechanism that relates different positions of a given sequence so as to generate a representation of the same sequence\n",
    "\n",
    "### Multi-head Attention\n",
    "- Multi-head attention extends the self-attention mechanism by performing multiple parallel self-attention operations, each focusing on different learned linear projections of the input. Multiple attention heads allow the model to capture different types of relationships and learn more fine-grained representations (eg: grammar, context, dependency, etc.)\n",
    "\n",
    "<img src=\"./assets/multihead_attention_notebook_3.png\">\n",
    "\n",
    "> Source: [Vasvani et. al.](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd448c92-cc9c-44c5-a408-d8f4e5706862",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Positional encoding is a technique used to incorporate the position of each token in the input sequence. It provides the model with information about the token's position without relying solely on the order of tokens.\n",
    "This additional aspect was required because transformers do not have the natural sequential setup of RNNs. In order to provide positional context, any encoding system should ideally have the following properties:\n",
    "\n",
    "- It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- It must be deterministic.\n",
    "\n",
    "<img src=\"./assets/positional_emb_notebook_3.png\">\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54409c-36e1-4836-8336-a63bcd32d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scienceplots\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(['science','no-latex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d2966-72aa-425c-ac73-51e7263a8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos,i,d_model,is_even=True):\n",
    "    \"\"\"\n",
    "    Method to generate positional encoding value\n",
    "    :param pos: position of the input\n",
    "    :param i: i-th dimension of the embedding\n",
    "    :param d_model: length of the embedding vector\n",
    "    :param is_even: if the position of the input is even or odd\n",
    "    \"\"\"\n",
    "    input_val = pos/np.power(10000,(2*i)/d_model)\n",
    "    if is_even:\n",
    "        return np.sin(input_val)\n",
    "    else:\n",
    "        return np.cos(input_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5c6ec-402e-4d61-af32-1f72815e448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters for visualisations\n",
    "pos = np.arange(0,10,0.1) #10 input words, stepping at 0.1 for smoothness only\n",
    "dimensions = np.arange(0,512) # dimensionality of the positional encoding (same as d_model by default)\n",
    "d_model = 512 # length of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ff1df-72d2-48e2-8d28-dd8dc2dd3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding for even positions\n",
    "even_pos_emb = [positional_encoding(pos,i,d_model) for i in dimensions] \n",
    "\n",
    "# positional encoding for off positions\n",
    "odd_pos_emb = #TODO: prepare positional embeddings for odd positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbea557-8d48-4280-9841-e99a1a754638",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dim = [0,16,32] # visualise only a few dimensions for clarity\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in dimensions:\n",
    "    if i in show_dim:\n",
    "        plt.plot(pos,even_pos_emb[i])\n",
    "        plt.plot(pos,odd_pos_emb[i])\n",
    "    plt.axvline(2,linestyle='--',c='black')\n",
    "plt.title(\"Positional Encodings\")        \n",
    "plt.xlabel(\"input positions\")\n",
    "plt.ylabel(\"encoding value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec21692-04c7-44f6-ba07-58d01bffe43e",
   "metadata": {},
   "source": [
    "## BERT-ology\n",
    "- BERT, or __[Bi-Directional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805)__, was presented by Devlin et al., a team at Google AI in 2018\n",
    "- Multi-task Learning: BERT also helped push the transfer-learning envelope in the NLP domain by showcasing how a pre-trained model can be fine-tuned for various tasks to provide state-of-the-art performance\n",
    "- BERT tweaked the usual Language Model objective to only predict next token based on past context by building context from both directions, i.e. the objective of predicting masked words along with next sentence prediction.\n",
    "\n",
    "\n",
    "<img src=\"./assets/bert_models_layout_notebook_3.jpeg\">\n",
    "\n",
    "> source [PLM Papers](https://github.com/thunlp/PLMpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a38e20-e963-4d1a-b65c-8e09a60a267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb245c9f-e147-49ef-a69c-e3defa41fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define some configs/constants\n",
    "DISTILBET_BASE_UNCASED_CHECKPOINT = \"distilbert/distilbert-base-uncased\"\n",
    "DISTILBET_QA_CHECKPOINT = \"distilbert/distilbert-base-uncased-distilled-squad\"\n",
    "DISTILBET_CLASSIFICATION_CHECKPOINT = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88cc2e-b8fb-48f9-8537-015420b4d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f241206-25ed-4021-9457-e1bdd665a029",
   "metadata": {},
   "source": [
    "### Predicting the Masked Token\n",
    "This was a unique objective when BERT was originally introduced as compared to usual NLP tasks such as classification. The objective requires us to prepare a dataset where we mask a certain percentage of input tokens and train the model to learn to predict those tokens. This objective turns out to be very effective in helping the model learn the nuances of language. \n",
    "\n",
    "In this first task we will test the pre-trained model against this objective itself. The model outputs a bunch of things such as the predicted token, encoded index of the predicted token/word along with a score which indicates the model's confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3efdda-c31f-438e-a19e-4593570dc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_pipeline = pipeline(\n",
    "    'fill-mask',\n",
    "    model=DISTILBET_BASE_UNCASED_CHECKPOINT,\n",
    "    device=DEVICE_ID\n",
    ")\n",
    "mlm_pipeline(\"Bangalore is the IT [MASK] of India.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540cf39-e8d9-412f-b339-49c93faf9723",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "This is an interesting NLP task and quite complex one as well. For this task, the model is provided input consisting of the context along with a question and it predicts the answer by selecting text from the context. The training setup for this task is a bit involved process, the following is an overview:\n",
    "- The training input as triplet of context, question and answer\n",
    "- This is transformed as combined input of the form ``[CLS]question[SEP]context[SEP]`` or ``[CLS]contex[SEP]question[SEP]`` with answer acting as the label\n",
    "- The model is trained to predict the start and end indices of the the corresponding answer for each input.\n",
    "\n",
    "\n",
    "For our current setting, we will leverage both _pretrained_ and _fine-tuned_ versions of **DistilBERT** via the _question-answering_ pipeline and understand the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae5a96-fd84-4b11-9943-5dfac261c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_ft_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=DISTILBET_QA_CHECKPOINT,\n",
    "    device=DEVICE_ID\n",
    ")\n",
    "qa_pt_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=#TODO: Set the pretrained \n",
    "    device=DEVICE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde485cf-1c21-4f34-a2d2-4d0130854905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a snippet about BERT like models from the module itself\n",
    "context = \"\"\"The key contribution from this set of models is the masked language modeling objective during the pre-training phase, where some tokens in the input are masked, and the model is trained to predict them (we will cover these in the upcoming section). Key works in this group of architectures are BERT, RoBERTa (or optimized BERT), DistilBERT (lighter and more efficient BERT), ELECTRA and ALBERT.\n",
    "In this notebook we will work through the task of Question Answering where our language model will learn to answer questions based on the context provided.\"\"\"\n",
    "question = \"What are the key works in this set of models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf038b7-e8c8-425e-bbd4-026895e73d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_qa_result= qa_ft_pipeline(\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "pt_qa_result= qa_pt_pipeline(\n",
    "    question=question,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd01d95-491a-4c22-afac-833de30b17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*55)\n",
    "print(f\"Context:{context}\")\n",
    "print(\"*\"*55)\n",
    "print(f\"Question:{question}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"Response from Fine-Tuned Model:\\n{ft_qa_result}\")\n",
    "print()\n",
    "print(f\"Response from Pretrained Model:\\n{pt_qa_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0ed16-e828-4ede-892e-04622bf82a35",
   "metadata": {},
   "source": [
    "# Generative Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7417f-425a-47ee-8707-7c9872cf8ecc",
   "metadata": {},
   "source": [
    "## Behold, its GPT (Generative pre-training)\n",
    "\n",
    "The first model in this series is called GPT, or Generative Pre-Training. It was released in [2018](https://openai.com/blog/language-unsupervised/), about the same time as the BERT model. The paper presents a task-agnostic architecture based on the ideas of transformers and unsupervised learning.\n",
    "\n",
    "- GPT is essentially a language model based on the __transformer-decoder__ \n",
    "- Introduction of large training datasets: __BookCorpus__ dataset contains over 7,000 unique, unpublished books across different genres\n",
    "- The GPT architecture makes use of 12 decoder blocks (as opposed to 6 in the original transformer) with 768-dimensional states and 12 self-attention heads each.\n",
    "\n",
    "\n",
    "### GPT-2\n",
    "- Radford et al. presented the GPT-2 model as part of their work titled [Language Models are Unsupervised Multi-task Learners in 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- The model achieves state-of-the-art performance in a few-shot setting\n",
    "- Similar to GPT, the secret sauce for GPT-2 is its dataset. The authors prepared a massive 40 GB dataset by crawling 45 million outbound links from a social networking site called Reddit.\n",
    "- The vocabulary was also expanded to cover 50,000 words and the context window was expanded to 1,024 tokens (as compared to 512 for GPT).\n",
    "\n",
    "\n",
    "### GPT-3\n",
    "- OpenAI published paper titled [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165) in May 2020. \n",
    "- This paper introduces the mammoth __175 billion-parameter GPT-3 model__.\n",
    "- Apart from more layers and parameters, this model made use of sparse attention\n",
    "- Dataset again played a key role, a 300 billion-token dataset based on existing datasets like Common Crawl (filtered for better content), WebText2 (a larger version of WebText used for GPT-2), Books1 and Books2, and the Wikipedia dataset was prepared for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383312da-e4a1-42d8-93c6-8c89f2a952d2",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "By far the most widely used application from the NLP world is language modeling. We use it daily on our phone keyboards, email applications and a ton of other places.\n",
    "\n",
    "In simple words, a language model takes certain text as input context to generate the next set of words as output. This is interesting because a language model tries to understand the input context, the language structure (though in a very naive way) to predict the next word(s). We use it in the form of text completion utilities on search engines, chat platforms, emails etc. all the time. Language models are a perfect real life application of NLP and showcase the power of RNNs.\n",
    "\n",
    "Language models can be developed train in different ways. The most common and widely used method is the sliding window approach. The model takes a small window of text as input and tried to predict the next word as the output. The following figure illustrates the same visually.\n",
    "\n",
    "<img src=\"./assets/lm_training_notebook_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc37334-041b-475b-b281-2b6c42902b27",
   "metadata": {},
   "source": [
    "### PreTrained GPT2 for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ddfd-4fe3-4aeb-afbd-1eb3f964e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73c89c-646f-47c7-8f36-ef4bc7b5b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative tasks are not available through MPS/Apple Silicon\n",
    "DEVICE = 'cpu'\n",
    "Tensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6949423-7182-4435-988c-89ed678e8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.#TODO: get pretrained GPT2 tokenizer\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909212-f6a3-46fd-8acc-5dc77224bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('The king of England is', return_tensors='pt').to(DEVICE)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210990-c4d2-4103-91ab-184addedba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
